{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd7b219",
   "metadata": {},
   "source": [
    "# Analysis of Tweets from a full archival search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72eebea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cdfd7f6-be30-400b-8073-ca8ea1455e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade twarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25281e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main folder and files (hashtags are the files).\n",
    "src = (r'C:\\Users\\Laser\\Desktop\\code_manuals') # put the path here. In windows, use: r'\\...'\n",
    "hashtags = ['Shell'] # Put the file name here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2981dee",
   "metadata": {},
   "source": [
    "Note: if the Tweets have already been downloaded and the Tweet data exists in the folder ```data``` as compressed ```.jsonl``` files, you can skip the \"Query tweets\" and \"Compress data\" steps and start processing at \"Decompress data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d1176f",
   "metadata": {},
   "source": [
    "## Collect Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d03a5",
   "metadata": {},
   "source": [
    "### Query tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b11e17a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"chmod\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n",
      "Die Syntax f�r den Dateinamen, Verzeichnisnamen oder die Datentr„gerbezeichnung ist falsch.\n"
     ]
    }
   ],
   "source": [
    "# Change file permissions such that execution is allowed\n",
    "! chmod +x r'C:\\Users\\Laser\\Desktop\\code_manuals\\queries\\shell.sh'\n",
    "\n",
    "# Run the query. Note: this can take a while, depending on the number of Tweets\n",
    "# that need to be downloaded\n",
    "! r'C:\\Users\\Laser\\Desktop\\code_manuals\\queries\\shell.sh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08455f32-b343-4ea9-86dd-52ca3c70d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x /media/s/Linux_storage/Analyse_Verkehrswende_Transformation/Code/Queries/Digitale_grüne_Bahn.sh\n",
    "! /media/s/Linux_storage/Analyse_Verkehrswende_Transformation/Code/Queries/Digitale_grüne_Bahn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa6e91-9d54-4b30-a533-2ce7d3238c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\Laser\\Desktop\\code_manuals\\queries>twarc2 search --start-time 2018-01-01 --end-time 2021-07-06 --archive \"greenwashing (to:shell)\" --max-results 100 > C:\\Users\\Laser\\Desktop\\code_manuals\\queries\\shell.jsonl\n",
    "\n",
    "C:\\Users\\Laser\\Desktop\\code_manuals\\queries>twarc2 search --start-time 2011-06-01 --end-time 2021-07-06 --archive \"greenwashing (to:db_bahn)\" --max-results 100 > C:\\Users\\Laser\\Desktop\\code_manuals\\queries\\db.jsonl\n",
    "\n",
    "C:\\Users\\Laser\\Desktop\\code_manuals\\queries>twarc2 search --start-time 2015-06-01 --end-time 2021-07-06 --archive \"greenwashing (to:volkswagen)\" --max-results 100 > C:\\Users\\Laser\\Desktop\\code_manuals\\queries\\vw.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b603904-7759-4d0d-b1da-576199b5ff9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-21d9b4bcf8e0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-21d9b4bcf8e0>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    twarc2 counts \"greenwashing OR Greenwashing OR GREENWASHING\" r'C:\\Users\\Laser\\Desktop\\code_manuals\\queries\\counts.csv' --archive --csv --granularity day\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "twarc2 counts \"greenwashing OR Greenwashing OR GREENWASHING\" greenwashing.csv --archive --csv --granularity day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1838f",
   "metadata": {},
   "source": [
    "### Compress data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e46b4",
   "metadata": {},
   "source": [
    "Note: under windows, .xz files can be decompressed for examply with [WinZIP](https://www.winzip.com/win/en/xz-file.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1ac97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the parameter \"-k\" keeps the original file\n",
    "! xz -k /media/s/Linux_storage/Analyse_Verkehrswende_Transformation/Data/Verkehrswende.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fccf34d",
   "metadata": {},
   "source": [
    "### Decompress data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! xz -d /media/s/Linux_storage/Analyse_Verkehrswende_Transformation/Data/Verkehrswende.jsonl.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5414039",
   "metadata": {},
   "source": [
    "### Convert to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3d24a",
   "metadata": {},
   "source": [
    "Removes duplicate tweets (by ID) but keeps referenced tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aed8e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/1.36M [00:00<?, ?B/s]\n",
      "100%|##########| 1.36M/1.36M [00:00<00:00, 2.58MB/s]\n",
      "\n",
      "\\u2139\\ufe0f\n",
      "Read 822 tweets from 5 lines. \n",
      "399 were referenced tweets, 203 were duplicates.\n",
      "Wrote 619 rows and output 90 of 90 input columns in the CSV.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! twarc2 csv --extra-input-columns \"in_reply_to_user.withheld.scope\" \"C:\\Users\\Laser\\Desktop\\code_manuals\\queries\\shell.jsonl\" \"C:\\Users\\Laser\\Desktop\\code_manuals\\queries\\shell.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e20ae49-5791-485a-b215-b5ab290d6396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/280k [00:00<?, ?B/s]\n",
      "100%|##########| 280k/280k [00:00<00:00, 1.86MB/s]\n",
      "\n",
      "\\u2139\\ufe0f\n",
      "Read 115 tweets from 1 lines. \n",
      "46 were referenced tweets, 20 were duplicates.\n",
      "Wrote 95 rows and output 90 of 90 input columns in the CSV.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! twarc2 csv --extra-input-columns \"in_reply_to_user.withheld.scope\" \"C:\\Users\\Laser\\Desktop\\code_manuals\\queries\\hm.jsonl\" \"C:\\Users\\Laser\\Desktop\\code_manuals\\queries\\hm.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f747c905-0169-46b0-bac9-e9fbf40a7cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/25.8k [00:00<?, ?B/s]\n",
      "100%|##########| 25.8k/25.8k [00:00<00:00, 506kB/s]\n",
      "\n",
      "\\u2139\\ufe0f\n",
      "Read 19 tweets from 1 lines. \n",
      "9 were referenced tweets, 2 were duplicates.\n",
      "Wrote 17 rows and output 90 of 90 input columns in the CSV.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! twarc2 csv --extra-input-columns \"in_reply_to_user.withheld.scope\" \"C:\\Users\\Laser\\Desktop\\code_manuals\\queries\\db.jsonl\" \"C:\\Users\\Laser\\Desktop\\code_manuals\\queries\\db.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2bbe60f-0e44-480c-9e8c-6ca9259835ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/1.53M [00:00<?, ?B/s]\n",
      "100%|##########| 1.53M/1.53M [00:00<00:00, 2.75MB/s]\n",
      "\n",
      "\\u2139\\ufe0f\n",
      "Read 1327 tweets from 8 lines. \n",
      "609 were referenced tweets, 492 were duplicates.\n",
      "Wrote 835 rows and output 90 of 90 input columns in the CSV.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! twarc2 csv --extra-input-columns \"in_reply_to_user.withheld.scope\" \"C:\\Users\\Laser\\Desktop\\code_manuals\\queries\\bahn.jsonl\" \"C:\\Users\\Laser\\Desktop\\code_manuals\\queries\\bahn.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2004290c",
   "metadata": {},
   "source": [
    "## Extract conversation IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b402255",
   "metadata": {},
   "source": [
    "Be aware that all necessary folders need to be existsting (case sensitive). Reduce chunksizes to get smaller .txt files. These can be quite long lists, maybe it makes more sense to work with the most active conversations. At least it is helpful to get the amount of tweets relative to conversations (the higher the tweets, the more intense the conversations are)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "385792e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_IDs(src, filename, chunks=False, chunksize=10000):\n",
    "    data = pd.read_csv(join(src, '{}.csv'.format(filename)), low_memory=False)\n",
    "    conversationIDs = data['conversation_id'].dropna().astype(int).unique()\n",
    "    print('{}: There are {} Tweets from {} conversations'\\\n",
    "              .format(filename, len(data), len(conversationIDs)))\n",
    "    \n",
    "    dst = join(src, 'conversation_IDs')\n",
    "    \n",
    "    if chunks:\n",
    "        N_chunks = len(conversationIDs) // chunksize\n",
    "        print(N_chunks)\n",
    "        for i in range(N_chunks):\n",
    "            ID_chunk = conversationIDs[i * chunksize : (i + 1) * chunksize]\n",
    "            np.savetxt(join(dst, '{}_ConversationIDs_{}_to_{}.txt'\\\n",
    "                .format(filename, i * chunksize, (i + 1) * chunksize)),\n",
    "                ID_chunk, fmt='%d')\n",
    "        np.savetxt(join(dst, '{}_ConversationIDs_{}_to_{}.txt'\\\n",
    "                .format(filename, N_chunks * chunksize, len(conversationIDs))),\n",
    "                conversationIDs[N_chunks * chunksize : ], fmt='%d')\n",
    "            \n",
    "    else:   \n",
    "        np.savetxt(join(dst, '{}_ConversationIDs.txt'.format(filename)),\n",
    "                   conversationIDs, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "306bca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verkehrswende: There are 47489 Tweets from 38917 conversations\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "get_conversation_IDs(src, 'Verkehrswende', chunks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd3665",
   "metadata": {},
   "source": [
    "## Extract Tweet IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f371c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Tweet_IDs(src, filename, chunks=False, chunksize=1000):\n",
    "    data = pd.read_csv(join(src, '{}.csv'.format(filename)), low_memory=False)\n",
    "    TweetIDs = data['id'].dropna().astype(int).unique()\n",
    "    print('{}: There are {} Tweets'\\\n",
    "              .format(filename, len(TweetIDs)))\n",
    "    \n",
    "    dst = join(src, 'tweet_IDs')\n",
    "    \n",
    "    if chunks:\n",
    "        N_chunks = len(TweetIDs) // chunksize\n",
    "        print(N_chunks)\n",
    "        for i in range(N_chunks):\n",
    "            ID_chunk = TweetIDs[i * chunksize : (i + 1) * chunksize]\n",
    "            np.savetxt(join(dst, '{}_TweetIDs_{}_to_{}.txt'\\\n",
    "                .format(filename, i * chunksize, (i + 1) * chunksize)),\n",
    "                ID_chunk, fmt='%d')\n",
    "        np.savetxt(join(dst, '{}_TweetIDs_{}_to_{}.txt'\\\n",
    "                .format(filename, N_chunks * chunksize, len(TweetIDs))),\n",
    "                TweetIDs[N_chunks * chunksize : ], fmt='%d')\n",
    "            \n",
    "    else:   \n",
    "        np.savetxt(join(dst, '{}_TweetIDs.txt'.format(filename)),\n",
    "                   TweetIDs, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e697a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IchBinHanna: There are 75910 Tweets\n"
     ]
    }
   ],
   "source": [
    "get_Tweet_IDs(src, 'IchBinHanna', chunks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909eacbd-9594-4258-b9b6-d3f7169aafb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
