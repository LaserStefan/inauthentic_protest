{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd7b219",
   "metadata": {},
   "source": [
    "# Analysis of Tweets from a full archival search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72eebea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cdfd7f6-be30-400b-8073-ca8ea1455e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade twarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25281e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main folder and files (hashtags are the files).\n",
    "src = '/media/s/Linux_storage/Analyse_Verkehrswende_Transformation/Data/Updates' # put the path here. In windows, use: r'\\...'\n",
    "hashtags = ['IchBinHanna'] # Put the file name here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2981dee",
   "metadata": {},
   "source": [
    "Note: if the Tweets have already been downloaded and the Tweet data exists in the folder ```data``` as compressed ```.jsonl``` files, you can skip the \"Query tweets\" and \"Compress data\" steps and start processing at \"Decompress data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d1176f",
   "metadata": {},
   "source": [
    "## Collect Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d03a5",
   "metadata": {},
   "source": [
    "### Query tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d311c",
   "metadata": {},
   "source": [
    "The goal is to get original posts about Verkehrswende AND Nachhaltigkeit (with substitues), without Retweets.\n",
    "Note: the queries are saved in separate files. I do this to make the data collection process reproducible by saving the exact query parameters for every data file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30657b6",
   "metadata": {},
   "source": [
    "Here is the main correlation: \n",
    "```(Verkehrswende OR Mobilitätswende ...)``` ```AND``` ```(Nachhaltigkeit OR Klimawandel ...)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b11e17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change file permissions such that execution is allowed\n",
    "! chmod +x /media/s/Linux_storage/Analyse_Verkehrswende_Transformation/Code/Queries/Bahnverkehr.sh\n",
    "\n",
    "# Run the query. Note: this can take a while, depending on the number of Tweets\n",
    "# that need to be downloaded\n",
    "! /media/s/Linux_storage/Analyse_Verkehrswende_Transformation/Code/Queries/Bahnverkehr.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c21cb",
   "metadata": {},
   "source": [
    "Now each following query *only* changes the first part, e.g. here for Elektromobilität:\n",
    "```(Elektromobilität OR Elektroauto ...)``` ```AND``` ```(Nachhaltigkeit OR Klimawandel...)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08455f32-b343-4ea9-86dd-52ca3c70d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x /media/s/Linux_storage/Analyse_Verkehrswende_Transformation/Code/Queries/Digitale_grüne_Bahn.sh\n",
    "! /media/s/Linux_storage/Analyse_Verkehrswende_Transformation/Code/Queries/Digitale_grüne_Bahn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1838f",
   "metadata": {},
   "source": [
    "### Compress data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e46b4",
   "metadata": {},
   "source": [
    "Note: under windows, .xz files can be decompressed for examply with [WinZIP](https://www.winzip.com/win/en/xz-file.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1ac97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the parameter \"-k\" keeps the original file\n",
    "! xz -k /media/s/Linux_storage/Analyse_Verkehrswende_Transformation/Data/Verkehrswende.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fccf34d",
   "metadata": {},
   "source": [
    "### Decompress data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! xz -d /media/s/Linux_storage/Analyse_Verkehrswende_Transformation/Data/Verkehrswende.jsonl.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5414039",
   "metadata": {},
   "source": [
    "### Convert to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3d24a",
   "metadata": {},
   "source": [
    "Removes duplicate tweets (by ID) but keeps referenced tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aed8e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 463k/463k [00:00<00:00, 2.88MB/s]\n",
      "\n",
      "ℹ️\n",
      "Read 505 tweets from 11 lines. \n",
      "254 were referenced tweets, 240 were duplicates.\n",
      "Wrote 265 rows and output 92 of 92 input columns in the CSV.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! twarc2 csv --extra-input-columns \"in_reply_to_user.withheld.scope\" /media/s/Linux_storage/Analyse_Verkehrswende_Transformation/Data/conversations_data/Hanna_threads.jsonl /media/s/Linux_storage/Analyse_Verkehrswende_Transformation/Data/conversations_data/Hanna_threads.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2004290c",
   "metadata": {},
   "source": [
    "## Extract conversation IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b402255",
   "metadata": {},
   "source": [
    "Be aware that all necessary folders need to be existsting (case sensitive). Reduce chunksizes to get smaller .txt files. These can be quite long lists, maybe it makes more sense to work with the most active conversations. At least it is helpful to get the amount of tweets relative to conversations (the higher the tweets, the more intense the conversations are)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "385792e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_IDs(src, filename, chunks=False, chunksize=10000):\n",
    "    data = pd.read_csv(join(src, '{}.csv'.format(filename)), low_memory=False)\n",
    "    conversationIDs = data['conversation_id'].dropna().astype(int).unique()\n",
    "    print('{}: There are {} Tweets from {} conversations'\\\n",
    "              .format(filename, len(data), len(conversationIDs)))\n",
    "    \n",
    "    dst = join(src, 'conversation_IDs')\n",
    "    \n",
    "    if chunks:\n",
    "        N_chunks = len(conversationIDs) // chunksize\n",
    "        print(N_chunks)\n",
    "        for i in range(N_chunks):\n",
    "            ID_chunk = conversationIDs[i * chunksize : (i + 1) * chunksize]\n",
    "            np.savetxt(join(dst, '{}_ConversationIDs_{}_to_{}.txt'\\\n",
    "                .format(filename, i * chunksize, (i + 1) * chunksize)),\n",
    "                ID_chunk, fmt='%d')\n",
    "        np.savetxt(join(dst, '{}_ConversationIDs_{}_to_{}.txt'\\\n",
    "                .format(filename, N_chunks * chunksize, len(conversationIDs))),\n",
    "                conversationIDs[N_chunks * chunksize : ], fmt='%d')\n",
    "            \n",
    "    else:   \n",
    "        np.savetxt(join(dst, '{}_ConversationIDs.txt'.format(filename)),\n",
    "                   conversationIDs, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "306bca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verkehrswende: There are 47489 Tweets from 38917 conversations\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "get_conversation_IDs(src, 'Verkehrswende', chunks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd3665",
   "metadata": {},
   "source": [
    "## Extract Tweet IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f371c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Tweet_IDs(src, filename, chunks=False, chunksize=1000):\n",
    "    data = pd.read_csv(join(src, '{}.csv'.format(filename)), low_memory=False)\n",
    "    TweetIDs = data['id'].dropna().astype(int).unique()\n",
    "    print('{}: There are {} Tweets'\\\n",
    "              .format(filename, len(TweetIDs)))\n",
    "    \n",
    "    dst = join(src, 'tweet_IDs')\n",
    "    \n",
    "    if chunks:\n",
    "        N_chunks = len(TweetIDs) // chunksize\n",
    "        print(N_chunks)\n",
    "        for i in range(N_chunks):\n",
    "            ID_chunk = TweetIDs[i * chunksize : (i + 1) * chunksize]\n",
    "            np.savetxt(join(dst, '{}_TweetIDs_{}_to_{}.txt'\\\n",
    "                .format(filename, i * chunksize, (i + 1) * chunksize)),\n",
    "                ID_chunk, fmt='%d')\n",
    "        np.savetxt(join(dst, '{}_TweetIDs_{}_to_{}.txt'\\\n",
    "                .format(filename, N_chunks * chunksize, len(TweetIDs))),\n",
    "                TweetIDs[N_chunks * chunksize : ], fmt='%d')\n",
    "            \n",
    "    else:   \n",
    "        np.savetxt(join(dst, '{}_TweetIDs.txt'.format(filename)),\n",
    "                   TweetIDs, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e697a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IchBinHanna: There are 75910 Tweets\n"
     ]
    }
   ],
   "source": [
    "get_Tweet_IDs(src, 'IchBinHanna', chunks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909eacbd-9594-4258-b9b6-d3f7169aafb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
